{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP10TwCbNFZJHHwvpCl0CQB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaZarrinmehr/API-Data-Extraction-and-Transformation-for-ERP-Integration/blob/main/Improving_Data_Integrity_Rectification_of_itmes_Database.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##API-Data-Extraction-and-Transformation-for-ERP-Integration"
      ],
      "metadata": {
        "id": "8Elt1xD7PUQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries and Settings"
      ],
      "metadata": {
        "id": "CTBhGZbtPAPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from itertools import islice\n",
        "from ast import literal_eval\n",
        "import time\n",
        "drive.mount(\"/content/drive\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "col_null_threshold=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfFQOe7BPAo1",
        "outputId": "824d9e1e-99ef-4a7a-bacc-01882fb45534"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "AYO0kD4sPHOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_resources_url(url):\n",
        "\n",
        "    \"\"\"\n",
        "    Retrieve all data from an API link generated by a report builder.\n",
        "\n",
        "    Args:\n",
        "    url (str): The API URL for fetching data from the report builder.\n",
        "\n",
        "    Returns:\n",
        "    df (pd.DataFrame): A DataFrame containing all the data retrieved\n",
        "                      from the specified API URL.\n",
        "\n",
        "    Example:\n",
        "    >>> url = \"https://api.example.com/v1/data?pageindex=\"\n",
        "    >>> data = get_all_resources_url(url)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the resources\n",
        "    resources = []\n",
        "\n",
        "    # Loop through all pages of resources\n",
        "    page_index = 0\n",
        "    while True:\n",
        "        # Get the resources with the specified page index\n",
        "        response = requests.get(url + str(page_index))\n",
        "\n",
        "        # Check for successful response\n",
        "        if response.status_code == 200:\n",
        "            json_data = json.loads(response.text)\n",
        "        else:\n",
        "            print(f'Error fetching data: {response.status_code}')\n",
        "            break\n",
        "\n",
        "        # Check if there are no more resources to retrieve\n",
        "        if json_data == \"No records found.\":\n",
        "            break\n",
        "\n",
        "        # Add the resources to the list\n",
        "        resources.extend(json_data['data'])\n",
        "\n",
        "        # Increment the page index for the next iteration\n",
        "        page_index += 1\n",
        "\n",
        "    # Create a pandas DataFrame from the list of resources\n",
        "    df = pd.DataFrame(resources)\n",
        "\n",
        "    # Return the DataFrame\n",
        "    return df\n",
        "\n",
        "def fetch_combine_store_data(urls, table_name, key):\n",
        "\n",
        "    \"\"\"\n",
        "    Fetch data from multiple URLs, combine the data, and store it in a CSV file.\n",
        "\n",
        "    This function takes a list of URLs and a table name as input. It fetches data from each URL\n",
        "    using the `get_all_resources_url` function, combines the data into a single DataFrame, and\n",
        "    saves the combined data to a CSV file with the specified table name.\n",
        "\n",
        "    Args:\n",
        "    urls (list): A list of strings, each representing a URL to fetch data from.\n",
        "    table_name (str): A string representing the name of the table to store the combined data in.\n",
        "    key: The column name to join on\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the combined data from all the URLs.\n",
        "\n",
        "    Raises:\n",
        "    ValueEroor: if key is nt in a data table\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty DataFrame to store the combined data\n",
        "    combined_data = get_all_resources_url(urls[0])\n",
        "\n",
        "    # Loop through the URLs, fetch data from each URL, and concatenate the data\n",
        "    for url in urls[1:]:\n",
        "        df = get_all_resources_url(url)\n",
        "        if key in df.columns:\n",
        "          combined_data = combined_data.merge(df, on=key)\n",
        "        else:\n",
        "          raise ValueError(f\"Key does not exist in the data provided by {url}\")\n",
        "\n",
        "    # Define the output file path\n",
        "    output_file_path = f\"/content/drive/My Drive/Projects/Shupper_Brickle/{table_name}_v2.csv\"\n",
        "\n",
        "    # Save the combined data to a CSV file\n",
        "    combined_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "def remove_high_null_columns(df, col_null_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Remove columns with a high percentage of null values from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame to be cleaned\n",
        "    col_null_threshold (float): Threshold for the percentage of null values to remove a column (default is 0.05)\n",
        "    table_name: table name\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Cleaned DataFrame with columns containing a high percentage of null values removed\n",
        "    \"\"\"\n",
        "    high_null_count = df.isnull().sum() > df.shape[0] * col_null_threshold\n",
        "    columns_to_retain = list(high_null_count[high_null_count == False].index)\n",
        "    df_clean = df[columns_to_retain]\n",
        "    return (df_clean)\n",
        "\n",
        "\n",
        "    return df_clean\n",
        "def clean_store_data(df, table_name, col_null_threshold=0.05):\n",
        "\n",
        "    df_clean = remove_high_null_columns(df, col_null_threshold)\n",
        "    output_file_path = f\"/content/drive/My Drive/Projects/Shupper_Brickle/{table_name}_Clean_v2.csv\"\n",
        "    df_clean.to_csv(output_file_path, index=False)\n",
        "    return (df_clean)"
      ],
      "metadata": {
        "id": "TYWQk4XIPGob"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleaning the Item Data and Storing it as a CSV for ERP Integration"
      ],
      "metadata": {
        "id": "VSXVOriJg8X7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s0S--RVugXTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7260979-c212-4d90-bf21-bd07a9a5eec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Items Shape = (2945, 32)\n"
          ]
        }
      ],
      "source": [
        "# List of URLs for the custom reports\n",
        "Items_urls = [\n",
        "    \"ENTER THE ITEMS_REPORT_1 API KEY HERE\",\n",
        "    \"ENTER THE ITEMS_REPORT_2 API KEY HERE\",\n",
        "    \"ENTER THE ITEMS_REPORT_3 API KEY HERE\"\n",
        "    ]\n",
        "\n",
        "Items = fetch_combine_store_data(urls=Items_urls, table_name=\"Items\", key=\"ID\")\n",
        "# Print the shape of the DataFrame\n",
        "print(f\"Items Shape = {Items.shape}\")\n",
        "\n",
        "Items_Clean = clean_store_data(Items, \"Items\", col_null_threshold)\n",
        "Items_Clean.head(2)\n",
        "\n",
        "# Create a working copy of the relevant columns from the cleaned items DataFrame\n",
        "df = Items_Clean[[\"ID\", \"ItemName\", \"ItemNumber\"]].copy()\n",
        "\n",
        "# Split \"ItemName\" at the first space and assign the first part (item number) to \"NewItemNumber\"\n",
        "df[\"NewItemNumber\"] = df.ItemName.apply(lambda x : x.split(' ',1)[0])\n",
        "\n",
        "# If the last character of \"NewItemNumber\" is \"-\" or \",\", remove it\n",
        "df[\"NewItemNumber\"] = df.NewItemNumber.apply(lambda x : x[:-1] if x[-1] in  [\"-\", \",\"] else x)\n",
        "\n",
        "# Split \"ItemName\" again and assign the second part (item name) to \"NewItemName\".\n",
        "# If there's no space in the string, retain the whole string.\n",
        "df[\"NewItemName\"] = df.ItemName.apply(lambda x : x.split(' ', 1)[1] if ' ' in x else x)\n",
        "\n",
        "# Remove leading and trailing spaces from \"NewItemName\" and \"NewItemNumber\"\n",
        "df[\"NewItemName\"] = df[\"NewItemName\"].str.strip()\n",
        "df[\"NewItemNumber\"] = df[\"NewItemNumber\"].str.strip()\n",
        "\n",
        "# Re-order columns for clarity\n",
        "df = df[[\"ID\", \"ItemName\", \"ItemNumber\", \"NewItemNumber\", \"NewItemName\"]]\n",
        "\n",
        "# Identify and display duplicates based on \"NewItemNumber\"\n",
        "ids = df[df[\"NewItemNumber\"].duplicated(keep=False)].sort_values(\"NewItemNumber\")[\"ID\"]\n",
        "#print(\"Duplicated rows:\")\n",
        "#display(df[df[\"NewItemNumber\"].duplicated(keep=False)].sort_values(\"NewItemNumber\"))\n",
        "\n",
        "# Create a new DataFrame to fix duplicates\n",
        "df3 = df.copy()\n",
        "df3['count'] = df3.groupby('NewItemNumber').cumcount() + 1\n",
        "df3['NewItemNumber'] = df3.apply(lambda row: row['NewItemNumber'] + '(' + str(row['count'])+ ')' if row['count'] != 1 else row['NewItemNumber'], axis=1)\n",
        "\n",
        "# Discard the temporary 'count' column\n",
        "df3 = df3.drop('count', axis=1)\n",
        "\n",
        "# Display the fixed duplicated rows for verification\n",
        "#print(\"Fixed duplicated rows:\")\n",
        "#display(df3[df3[\"ID\"].isin(ids)].sort_values(\"NewItemNumber\"))\n",
        "\n",
        "# Show a sample from the final DataFrame\n",
        "#display(df3.head(10))\n",
        "\n",
        "# Save the final DataFrame as a CSV file\n",
        "df3.to_csv(\"/content/drive/MyDrive/ItemsNameNumberUpdateRef.csv\", index= False)\n",
        "\n",
        "#Upload the df3 to striven"
      ]
    }
  ]
}
