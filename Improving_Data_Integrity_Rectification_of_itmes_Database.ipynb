{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TgHcqwm6dJjl3HbdPvJ0SqzjSD8kY30l",
      "authorship_tag": "ABX9TyMIcdgT+of7qdDBI/L9eNoV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##API-Data-Extraction-and-Transformation-for-ERP-Integration"
      ],
      "metadata": {
        "id": "8Elt1xD7PUQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script is a Python-based data extraction, transformation, and loading (ETL) project. Here is what the script does:\n",
        "\n",
        "Data Extraction (API calls):\n",
        "\n",
        "It defines a function get_all_resources_url(url). This function fetches all the data from an API URL provided by a report builder. It fetches all pages of data until there's no more data to fetch. It stores the data in a list, then converts this list to a pandas DataFrame, and returns it. The function fetch_combine_store_data(urls, table_name, key) uses the get_all_resources_url(url) function to fetch data from a list of URLs, combines this data into a single DataFrame, and stores this DataFrame as a CSV file on the specified path. Data Transformation (Cleaning and reshaping the data):\n",
        "\n",
        "The remove_high_null_columns(df, col_null_threshold=0.05) function removes columns from a DataFrame that have a high percentage of null values based on a specified threshold. It returns a cleaned DataFrame. The function clean_store_data(df, table_name, col_null_threshold=0.05) utilizes the above function to clean a DataFrame, then it stores the cleaned DataFrame as a CSV file on the specified path. Following the cleaning, it performs additional transformations on the DataFrame like splitting column values, removing certain characters, and adjusting leading and trailing spaces. It also deals with duplicates in the data by adding a count number in brackets for duplicated values. Data Loading:\n",
        "\n",
        "The final DataFrame is saved to a CSV file. As a part of this ETL pipeline, the cleaned and transformed data is prepared for uploading into another system (for example, an ERP system or a database).\n",
        "\n",
        "The purpose of this script is to pull data from multiple APIs, clean and transform the data as needed, and finally, prepare it for loading into another system or application. This kind of process is common in data analysis and data science projects, where data needs to be moved and transformed between different systems."
      ],
      "metadata": {
        "id": "LZdZAMnkVuBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Libraries and Settings"
      ],
      "metadata": {
        "id": "CTBhGZbtPAPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import base64\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from itertools import islice\n",
        "from ast import literal_eval\n",
        "import time\n",
        "drive.mount(\"/content/drive\")\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "col_null_threshold=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfFQOe7BPAo1",
        "outputId": "c1e26e4a-918c-486b-98fc-01bb950599d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions"
      ],
      "metadata": {
        "id": "AYO0kD4sPHOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_resources_url(url):\n",
        "\n",
        "    \"\"\"\n",
        "    Retrieve all data from an API link generated by a report builder.\n",
        "\n",
        "    Args:\n",
        "    url (str): The API URL for fetching data from the report builder.\n",
        "\n",
        "    Returns:\n",
        "    df (pd.DataFrame): A DataFrame containing all the data retrieved\n",
        "                      from the specified API URL.\n",
        "\n",
        "    Example:\n",
        "    >>> url = \"https://api.example.com/v1/data?pageindex=\"\n",
        "    >>> data = get_all_resources_url(url)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty list to store the resources\n",
        "    resources = []\n",
        "\n",
        "    # Loop through all pages of resources\n",
        "    page_index = 0\n",
        "    while True:\n",
        "        # Get the resources with the specified page index\n",
        "        response = requests.get(url + str(page_index))\n",
        "\n",
        "        # Check for successful response\n",
        "        if response.status_code == 200:\n",
        "            json_data = json.loads(response.text)\n",
        "        else:\n",
        "            print(f'Error fetching data: {response.status_code}')\n",
        "            break\n",
        "\n",
        "        # Check if there are no more resources to retrieve\n",
        "        if json_data == \"No records found.\":\n",
        "            break\n",
        "\n",
        "        # Add the resources to the list\n",
        "        resources.extend(json_data['data'])\n",
        "\n",
        "        # Increment the page index for the next iteration\n",
        "        page_index += 1\n",
        "\n",
        "    # Create a pandas DataFrame from the list of resources\n",
        "    df = pd.DataFrame(resources)\n",
        "\n",
        "    # Return the DataFrame\n",
        "    return df\n",
        "\n",
        "def fetch_combine_store_data(urls, table_name, key):\n",
        "\n",
        "    \"\"\"\n",
        "    Fetch data from multiple URLs, combine the data, and store it in a CSV file.\n",
        "\n",
        "    This function takes a list of URLs and a table name as input. It fetches data from each URL\n",
        "    using the `get_all_resources_url` function, combines the data into a single DataFrame, and\n",
        "    saves the combined data to a CSV file with the specified table name.\n",
        "\n",
        "    Args:\n",
        "    urls (list): A list of strings, each representing a URL to fetch data from.\n",
        "    table_name (str): A string representing the name of the table to store the combined data in.\n",
        "    key: The column name to join on\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: A DataFrame containing the combined data from all the URLs.\n",
        "\n",
        "    Raises:\n",
        "    ValueEroor: if key is nt in a data table\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize an empty DataFrame to store the combined data\n",
        "    combined_data = get_all_resources_url(urls[0])\n",
        "\n",
        "    # Loop through the URLs, fetch data from each URL, and concatenate the data\n",
        "    for url in urls[1:]:\n",
        "        df = get_all_resources_url(url)\n",
        "        if key in df.columns:\n",
        "          combined_data = combined_data.merge(df, on=key)\n",
        "        else:\n",
        "          raise ValueError(f\"Key does not exist in the data provided by {url}\")\n",
        "\n",
        "    # Define the output file path\n",
        "    output_file_path = f\"/content/drive/My Drive/Projects/Shupper_Brickle/{table_name}_v2.csv\"\n",
        "\n",
        "    # Save the combined data to a CSV file\n",
        "    combined_data.to_csv(output_file_path, index=False)\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "def remove_high_null_columns(df, col_null_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Remove columns with a high percentage of null values from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame to be cleaned\n",
        "    col_null_threshold (float): Threshold for the percentage of null values to remove a column (default is 0.05)\n",
        "    table_name: table name\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Cleaned DataFrame with columns containing a high percentage of null values removed\n",
        "    \"\"\"\n",
        "    high_null_count = df.isnull().sum() > df.shape[0] * col_null_threshold\n",
        "    columns_to_retain = list(high_null_count[high_null_count == False].index)\n",
        "    df_clean = df[columns_to_retain]\n",
        "    return (df_clean)\n",
        "\n",
        "\n",
        "    return df_clean\n",
        "def clean_store_data(df, table_name, col_null_threshold=0.05):\n",
        "\n",
        "    df_clean = remove_high_null_columns(df, col_null_threshold)\n",
        "    output_file_path = f\"/content/drive/My Drive/Projects/Shupper_Brickle/{table_name}_Clean_v2.csv\"\n",
        "    df_clean.to_csv(output_file_path, index=False)\n",
        "    return (df_clean)"
      ],
      "metadata": {
        "id": "TYWQk4XIPGob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cleaning the Item Data and Storing it as a CSV for ERP Integration"
      ],
      "metadata": {
        "id": "VSXVOriJg8X7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0S--RVugXTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7260979-c212-4d90-bf21-bd07a9a5eec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Items Shape = (2945, 32)\n"
          ]
        }
      ],
      "source": [
        "# List of URLs for the custom reports\n",
        "\n",
        "Items_urls = [\n",
        "\n",
        "    ]\n",
        "\n",
        "Items = fetch_combine_store_data(urls=Items_urls, table_name=\"Items\", key=\"ID\")\n",
        "# Print the shape of the DataFrame\n",
        "print(f\"Items Shape = {Items.shape}\")\n",
        "\n",
        "Items_Clean = clean_store_data(Items, \"Items\", col_null_threshold)\n",
        "Items_Clean.head(2)\n",
        "\n",
        "# Create a working copy of the relevant columns from the cleaned items DataFrame\n",
        "df = Items_Clean[[\"ID\", \"ItemName\", \"ItemNumber\"]].copy()\n",
        "\n",
        "# Split \"ItemName\" at the first space and assign the first part (item number) to \"NewItemNumber\"\n",
        "df[\"NewItemNumber\"] = df.ItemName.apply(lambda x : x.split(' ',1)[0])\n",
        "\n",
        "# If the last character of \"NewItemNumber\" is \"-\" or \",\", remove it\n",
        "df[\"NewItemNumber\"] = df.NewItemNumber.apply(lambda x : x[:-1] if x[-1] in  [\"-\", \",\"] else x)\n",
        "\n",
        "# Split \"ItemName\" again and assign the second part (item name) to \"NewItemName\".\n",
        "# If there's no space in the string, retain the whole string.\n",
        "df[\"NewItemName\"] = df.ItemName.apply(lambda x : x.split(' ', 1)[1] if ' ' in x else x)\n",
        "\n",
        "# Remove leading and trailing spaces from \"NewItemName\" and \"NewItemNumber\"\n",
        "df[\"NewItemName\"] = df[\"NewItemName\"].str.strip()\n",
        "df[\"NewItemNumber\"] = df[\"NewItemNumber\"].str.strip()\n",
        "\n",
        "# Re-order columns for clarity\n",
        "df = df[[\"ID\", \"ItemName\", \"ItemNumber\", \"NewItemNumber\", \"NewItemName\"]]\n",
        "\n",
        "# Identify and display duplicates based on \"NewItemNumber\"\n",
        "ids = df[df[\"NewItemNumber\"].duplicated(keep=False)].sort_values(\"NewItemNumber\")[\"ID\"]\n",
        "#print(\"Duplicated rows:\")\n",
        "#display(df[df[\"NewItemNumber\"].duplicated(keep=False)].sort_values(\"NewItemNumber\"))\n",
        "\n",
        "# Create a new DataFrame to fix duplicates\n",
        "df3 = df.copy()\n",
        "df3['count'] = df3.groupby('NewItemNumber').cumcount() + 1\n",
        "df3['NewItemNumber'] = df3.apply(lambda row: row['NewItemNumber'] + '(' + str(row['count'])+ ')' if row['count'] != 1 else row['NewItemNumber'], axis=1)\n",
        "\n",
        "# Discard the temporary 'count' column\n",
        "df3 = df3.drop('count', axis=1)\n",
        "\n",
        "# Display the fixed duplicated rows for verification\n",
        "#print(\"Fixed duplicated rows:\")\n",
        "#display(df3[df3[\"ID\"].isin(ids)].sort_values(\"NewItemNumber\"))\n",
        "\n",
        "# Show a sample from the final DataFrame\n",
        "#display(df3.head(10))\n",
        "\n",
        "# Save the final DataFrame as a CSV file\n",
        "df3.to_csv(\"/content/drive/MyDrive/Projects/Shupper_Brickle/Project 15 Items Data/items-data-v1.csv\", index= False)\n",
        "\n",
        "#Upload the df3 to striven"
      ]
    }
  ]
}